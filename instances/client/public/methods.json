{
    "service": "methods",
    "enabled": true,
    "data": [
        {
            "name": "Global Color Transfer",
            "key": "GLO",
            "type": "Color Transfer",
            "author": "Erik Reinhard, Michael Ashikhmin, Bruce Gooch, Peter Shirley",
            "year": 2001,
            "publication": "Color Transfer between Images",
            "doi": "https://doi.org/10.1109/38.946629",
            "scientificVenue": "IEEE Computer Graphics and Applications (CG&A)",
            "abstract": "We use a simple statistical analysis to impose one image's color characteristics on another. We can achieve color correction by choosing an appropriate source image and apply its characteristic to another image.",
            "datatypes": ["Image", "PointCloud", "Mesh", "Video", "VolumetricVideo", "LightField", "GaussianSplatting"],
            "local": false,
            "enabled": true
        },
        {
            "name": "BasicColorCategoryTransfer",
            "key": "BCC",
            "type": "Color Transfer",
            "author": "Youngha Chang, Suguru Saito, Masayuko Nakajima",
            "year": 2003,
            "publication": "A Framework for Transfer Colors Based on the Basic Color Categories",
            "doi": "https://doi.org/10.1109/CGI.2003.1214463",
            "scientificVenue": "Proceedings of Computer Graphics International Conference (CGI)",
            "abstract": "Usually, paintings are more appealing than photographic images. This is because paintings have styles. This style can be distinguished by looking at elements such as motif, color, shape deformation and brush texture. We focus on the effect of color element and devise a method for transforming the color of an input photograph according to a reference painting. To do this, we consider basic color category concepts in the color transformation process. By doing so, we achieve large but natural color transformations of an image.",
            "datatypes": [],
            "local": false,
            "enabled": true
        },
        {
            "name": "PdfColorTransfer",
            "key": "PDF",
            "type": "Color Transfer",
            "author": "Francois Pitie, Anil C. Kokaram, Rozenn Dahyot",
            "year": 2005,
            "publication": "N-dimensional probability density function transfer and its application to color transfer",
            "doi": "https://doi.org/10.1109/ICCV.2005.166",
            "scientificVenue": "IEEE International Conference on Computer Vision (ICCV)",
            "abstract": "This article proposes an original method to estimate a continuous transformation that maps one N-dimensional distribution to another. The method is iterative, non-linear, and is shown to converge. Only 1D marginal distribution is used in the estimation process, hence involving low computation costs. As an illustration this mapping is applied to color transfer between two images of different contents. The paper also serves as a central focal point for collecting together the research activity in this area and relating it to the important problem of automated color grading.",
            "datatypes": [],
            "local": false,
            "enabled": true
        },
        {
            "name": "CorrelatedColorSpaceTransfer",
            "key": "CCS",
            "type": "Color Transfer",
            "author": "Xuezhong Xiao, Lizhuang Ma",
            "year": 2006,
            "publication": "Color Transfer in Correlated Color Space",
            "doi": "https://doi.org/10.1145/1128923.1128974",
            "scientificVenue": "Proceedings of the ACM International Conference on Virtual Reality Continuum and its Applications",
            "abstract": "In this paper we present a process called color transfer which can borrow one image's color characteristics from another. Recently Reinhard and his colleagues reported a pioneering work of color transfer. Their technology can produce very believable results, but has to transform pixel values from RGB to lαβ . Inspired by their work, we advise an approach which can directly deal with the color transfer in any 3D space. From the view of statistics, we consider pixel's value as a three- dimension stochastic variable and an image as a set of samples, so the correlations between three components can be measured by covariance. Our method imports covariance between three compo- nents of pixel values while calculate the mean along each of the three axes. Then we decompose the covariance matrix using SVD algorithm and get a rotation matrix. Finally we can scale, rotate and shift pixel data of target image to t data points' cluster of source image in the current color space and get resultant image which takes on source image's look and feel. Besides the global processing, a swatch-based method is introduced in order to manipulate images' color more elaborately. Experimental results conrm the validity and usefulness of our method.",
            "datatypes": [],
            "local": false,
            "enabled": true
        },
        {
            "name": "MongeKLColorTransfer",
            "key": "MKL",
            "type": "Color Transfer",
            "author": "Erik Reinhard, Michael Ashikhmin, Bruce Gooch, Peter Shirley",
            "year": 2007,
            "publication": "The Linear Monge-Kantorovitch Linear Colour Mapping for Example-Based Colour Transfer",
            "doi": "https://doi.org/10.1049/cp:20070055",
            "scientificVenue": "European Conference on Visual Media Production",
            "abstract": "A common task in image editing is to change the colours of a picture to match the desired colour grade of another picture. Finding the correct colour mapping is tricky because it involves numerous interrelated operations, like balancing the colours, mixing the colour channels or adjusting the contrast. Recently, a number of automated tools have been proposed to find an adequate one-to-one colour mapping. The focus in this paper is on finding the best linear colour transformation. Linear transformations have been proposed in the literature but independently. The aim of this paper is thus to establish a common mathematical background to all these methods. Also, this paper proposes a novel transformation, which is derived from the Monge-Kantorovitch theory of mass transportation. The proposed solution is optimal in the sense that it minimises the amount of changes in the picture colours. It favourably compares theoretically and experimentally with other techniques for various images and under various colour spaces.",
            "datatypes": [],
            "local": false,
            "enabled": true
        },
        {
            "name": "GradientPreservingColorTransfer",
            "key": "GPC",
            "type": "Color Transfer",
            "author": "Xuezhong Xiao, Lizhuang Ma",
            "year": 2009,
            "publication": "Gradient-Preserving Color Transfer",
            "doi": "https://doi.org/10.1111/j.1467-8659.2009.01566.x",
            "scientificVenue": "IEEE Computer Graphics and Applications",
            "abstract": "Color transfer is an image processing technique which can produce a new image combining one source image's contents with another image's color style. While being able to produce convincing results, however, Reinhard et al.'s pioneering work has two problems—mixing up of colors in different regions and the fidelity problem. Many local color transfer algorithms have been proposed to resolve the first problem, but the second problem was paid few attentions. In this paper, a novel color transfer algorithm is presented to resolve the fidelity problem of color transfer in terms of scene details and colors. It's well known that human visual system is more sensitive to local intensity differences than to intensity itself. We thus consider that preserving the color gradient is necessary for scene fidelity. We formulate the color transfer problem as an optimization problem and solve it in two steps—histogram matching and a gradient-preserving optimization. Following the idea of the fidelity in terms of color and gradient, we also propose a metric for objectively evaluating the performance of example-based color transfer algorithms. The experimental results show the validity and high fidelity of our algorithm and that it can be used to deal with local color transfer.",
            "datatypes": [],
            "local": false,
            "enabled": true
        },
        {
            "name": "FuzzyColorTransfer",
            "key": "FUZ",
            "type": "Color Transfer",
            "author": "XiaoYan Qian, BangFeng Wang, Lei Han",
            "year": 2010,
            "publication": "An efficient fuzzy clustering-based color transfer method",
            "doi": "https://doi.org/10.1109/FSKD.2010.5569560",
            "scientificVenue": "International Conference on Fuzzy Systems and Knowledge Discovery",
            "abstract": "Each image has its own color content that greatly influences the perception of human observer. Recently, color transfer among different images has been under investigation. In this paper, after a brief review on the few efficient works performed in the field, a novel fuzzy clustering based color transfer method is proposed. The proposed method accomplishes the transformation based on a set of corresponding fuzzy clustering algorithm-selected regions in images along with membership degree factors. Results show the presented algorithm is highly automatically and more effective.",
            "datatypes": [],
            "local": false,
            "enabled": true
        },
        {
            "name": "NeuralStyleTransfer",
            "key": "NST",
            "type": "Style Transfer",
            "author": "Leon A. Gatys, Alexander S. Ecker, Matthias Bethge",
            "year": 2015,
            "publication": "A Neural Algorithm of Artistic Style",
            "doi": "https://doi.org/10.48550/arXiv.1508.06576",
            "scientificVenue": "arXiv",
            "abstract": "In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.",
            "datatypes": [],
            "local": false,
            "enabled": true
        },
        {
            "name": "Deep Photo Style Transfer",
            "key": "DPT",
            "type": "Style Transfer",
            "author": "Fujun Luan, Sylvain Paris, Eli Shechtman, Kavita Bala",
            "year": 2017,
            "publication": "Deep Photo Style Transfer",
            "doi": "https://doi.org/10.48550/arXiv.1703.07511",
            "scientificVenue": "arXiv",
            "abstract": "This paper introduces a deep-learning approach to photographic style transfer that handles a large variety of image content while faithfully transferring the reference style. Our approach builds upon the recent work on painterly transfer that separates style from the content of an image by considering different layers of a neural network. However, as is, this approach is not suitable for photorealistic style transfer. Even when both the input and reference images are photographs, the output still exhibits distortions reminiscent of a painting. Our contribution is to constrain the transformation from the input to the output to be locally affine in colorspace, and to express this constraint as a custom fully differentiable energy term. We show that this approach successfully suppresses distortion and yields satisfying photorealistic style transfers in a broad variety of scenarios, including transfer of the time of day, weather, season, and artistic edits.",
            "datatypes": [],
            "local": false,
            "enabled": true
        },
        {
            "name": "TpsColorTransfer",
            "key": "TPS",
            "type": "Color Transfer",
            "author": "Mairéad Grogan, Rozenn Dahyot",
            "year": 2019,
            "publication": "L2 Divergence for robust colour transfer",
            "doi": "https://doi.org/10.1016/j.cviu.2019.02.002",
            "scientificVenue": "Computer Vision and Image Understanding",
            "abstract": "Optimal Transport (OT) is a very popular framework for performing colour transfer in images and videos. We have proposed an alternative framework where the cost function used for inferring a parametric transfer function is defined as the robust L2 divergence between two probability density functions (Grogan and Dahyot, 2015). In this paper, we show that our approach combines many advantages of state of the art techniques and outperforms many recent algorithms as measured quantitatively with standard quality metrics, and qualitatively using perceptual studies (Grogan and Dahyot, 2017). Mathematically, our formulation is presented in contrast to the OT cost function that shares similarities with our cost function. Our formulation, however, is more flexible as it allows colour correspondences that may be available to be taken into account and performs well despite potential occurrences of correspondence outlier pairs. Our algorithm is shown to be fast, robust and it easily allows for user interaction providing freedom for artists to fine tune the recoloured images and videos (Grogan et al., 2017).",
            "datatypes": [],
            "local": false,
            "enabled": true
        },
        {
            "name": "HistogramAnalogy",
            "key": "HIS",
            "type": "Color Transfer",
            "author": "Junyong Lee, Hyeongseok Son, Gunhee Lee, Jonghyeop Lee, Sunghyun Cho, Seungyong Lee",
            "year": 2020,
            "publication": "Deep Color Transfer using Histogram Analogy",
            "doi": "https://doi.org/10.1007/s00371-020-01921-6",
            "scientificVenue": "The Visual Computer: International Journal of Computer Graphics",
            "abstract": "We propose a novel approach to transferring the color of a reference image to a given source image. Although there can be diverse pairs of source and reference images in terms of content and composition similarity, previous methods are not capable of covering the whole diversity. To resolve this limitation, we propose a deep neural network that leverages color histogram analogy for color transfer. A histogram contains essential color information of an image, and our network utilizes the analogy between the source and reference histograms to modulate the color of the source image with abstract color features of the reference image. In our approach, histogram analogy is exploited basically among the whole images, but it can also be applied to semantically corresponding regions in the case that the source and reference images have similar contents with different compositions. Experimental results show that our approach effectively transfers the reference colors to the source images in a variety of settings. We also demonstrate a few applications of our approach, such as palette-based recolorization, color enhancement, and color editing.",
            "datatypes": [],
            "local": false,
            "enabled": true
        },
        {
            "name": "PSNetStyleTransfer",
            "key": "PSN",
            "type": "Style Transfer",
            "author": "Xu Cao, Weimin Wang, Katashi Nagao, Ryosuke Nakamura",
            "year": 2020,
            "publication": "PSNet: A Style Transfer Network for Point Cloud Stylization on Geometry and Color",
            "doi": "https://doi.org/10.1109/WACV45572.2020.9093513",
            "scientificVenue": "IEEE Winter Conference on Applications of Computer Vision (WACV)",
            "abstract": "We propose a neural style transfer method for colored point clouds which allows stylizing the geometry and/or color property of a point cloud from another. The stylization is achieved by manipulating the content representations and Gram-based style representations extracted from a pretrained PointNet-based classification network for colored point clouds. As Gram-based style representation is invariant to the number or the order of points, the style can also be an image in the case of stylizing the color property of a point cloud by merely treating the image as a set of pixels. Experimental results and analysis demonstrate the capability of the proposed method for stylizing a point cloud either from another point cloud or an image.",
            "datatypes": [],
            "local": false,
            "enabled": true
        },
        {
            "name": "Example-Based Colour Transfer",
            "key": "EB3",
            "type": "Color Transfer",
            "author": "Ific Goudé, Rémi Cozot, Olivier Le Meur, Kadi Bouatouch",
            "year": 2021,
            "publication": "Example-Based Colour Transfer for 3D Point Clouds",
            "doi": "https://doi.org/10.1111/cgf.14388",
            "scientificVenue": "Computer Graphics Forum",
            "abstract": "Example-based colour transfer between images, which has raised a lot of interest in the past decades, consists of transferring the colour of an image to another one. Many methods based on colour distributions have been proposed, and more recently, the efficiency of neural networks has been demonstrated again for colour transfer problems. In this paper, we propose a new pipeline with methods adapted from the image domain to automatically transfer the colour from a target point cloud to an input point cloud. These colour transfer methods are based on colour distributions and account for the geometry of the point clouds to produce a coherent result. The proposed methods rely on simple statistical analysis, are effective, and succeed in transferring the colour style from one point cloud to another. The qualitative results of the colour transfers are evaluated and compared with existing methods.",
            "datatypes": [],
            "local": false,
            "enabled": true
        },
        {
            "name": "CamsTransfer",
            "key": "CAM",
            "type": "Style Transfer",
            "author": "Mahmoud Afifi, Abdullah Abuolaim, Mostafa Hussien, Marcus A. Brubaker, Michael S. Brown",
            "year": 2021,
            "publication": "CAMS: Color-Aware Multi-Style Transfer",
            "doi": "https://doi.org/10.48550/arXiv.2106.13920",
            "scientificVenue": "arXiv",
            "abstract": "Image style transfer aims to manipulate the appearance of a source image, or content image, to share similar texture and colors of a target style image. Ideally, the style transfer manipulation should also preserve the semantic content of the source image. A commonly used approach to assist in transferring styles is based on Gram matrix optimization. One problem of Gram matrix-based optimization is that it does not consider the correlation between colors and their styles. Specifically, certain textures or structures should be associated with specific colors. This is particularly challenging when the target style image exhibits multiple style types. In this work, we propose a color-aware multi-style transfer method that generates aesthetically pleasing results while preserving the style-color correlation between style and generated images. We achieve this desired outcome by introducing a simple but efficient modification to classic Gram matrix-based style transfer optimization. A nice feature of our method is that it enables the users to manually select the color associations between the target style and content image for more transfer flexibility. We validated our method with several qualitative comparisons, including a user study conducted with 30 participants. In comparison with prior work, our method is simple, easy to implement, and achieves visually appealing results when targeting images that have multiple styles.",
            "datatypes": [],
            "local": false,
            "enabled": true
        },
        {
            "name": "ReHistoGAN",
            "key": "RHG",
            "type": "Color Transfer",
            "author": "Mahmoud Afifi, Marcus A. Brubaker, Michael S. Brown",
            "year": 2021,
            "publication": "HistoGAN: Controlling Colors of GAN-Generated and Real Images via Color Histograms",
            "doi": "https://doi.org/10.1109/CVPR46437.2021.00785",
            "scientificVenue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "abstract": "While generative adversarial networks (GANs) can successfully produce high-quality images, they can be challenging to control. Simplifying GAN-based image generation is critical for their adoption in graphic design and artistic work. This goal has led to significant interest in methods that can intuitively control the appearance of images generated by GANs. In this paper, we present HistoGAN, a color histogram-based method for controlling GAN-generated images’ colors. We focus on color histograms as they provide an intuitive way to describe image color while remaining decoupled from domain-specific semantics. Specifically, we introduce an effective modification of the recent StyleGAN architecture [31] to control the colors of GAN-generated images specified by a target color histogram feature. We then describe how to expand HistoGAN to recolor real images. For image recoloring, we jointly train an encoder network along with HistoGAN. The recoloring model, ReHistoGAN, is an unsupervised approach trained to encourage the network to keep the original image’s content while changing the colors based on the given target histogram. We show that this histogram-based approach offers a better way to control GAN-generated and real images’ colors while producing more compelling results compared to existing alternative strategies.",
            "datatypes": [],
            "local": false,
            "enabled": true
        }
    ]
}